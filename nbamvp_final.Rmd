---
title: "NBA MVP Predictions"
author: "Ian Bogley, Jonas Bowman"
date: "2/22/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE)
```

A constant in the sporting world is attempting to predict various metrics associated with these unpredictable games. From march madness, where individuals make their bets for how college playoffs will turn out to sports fans arguing over whether their team will succeed.

An interesting application of this may be in the MVP and All-star selections, where players are chosen based off of voting systems associated with media spectators and individual ones. One question is whether we might be able to use computer algorithms and data analysis to predict how voters chose these prestigious awards.

Let's use standard machine learning techniques to attempt predictions on an NBA season's MVP and all-star selections. To start, lets load the packages we need for this project.

```{r intro}
###PACKAGES###
library(pacman)
p_load(rvest,tidyverse,janitor,tidymodels,broom,
       rpart,rpart.plot)


###DATA###
##Read in data, use janitor::clean_names for renaming columns

#background info
playerstats_df <- read.csv("data/player_data.csv") %>% clean_names()%>%rename(player = name)

#physical characteristics
players_df <- read.csv("data/Players.csv") %>% clean_names() %>%
  mutate(player = gsub(" +$","",gsub("[^[:alpha:]]"," ",player)))

#season stats
seasonstats_df <- read.csv("data/Seasons_Stats.csv") %>%
  #Change year format to be more specific (from xxxx-yy to xxxx-yyyy)
  mutate(season = paste(Year-1,Year,sep = "-")) %>%
  #reorder columns for convenience
  .[,c(1,2,54,3:53)] %>% clean_names() %>%
  #Only considering data after the 1985 season
  filter(year>1985) %>%
  #Clean up player names
  mutate(player = gsub(" +$","",gsub("[^[:alpha:]]"," ",player)))

#Read in wikipedia scrapped data
#MVP data
mvp <- read_html("https://en.wikipedia.org/wiki/NBA_Most_Valuable_Player_Award") %>%
  html_nodes(xpath = "//*[@id='mw-content-text']/div[1]/table[4]") %>%
  html_table() %>% data.frame() %>% clean_names() %>%
  #clean up season and player variables
  mutate(season = gsub("[^[:alnum:]]","-19",season),
         player = gsub(" +$","",gsub("[^[:alpha:]]"," ",player))) %>%
  #rename player variable to ensure smooth joining with other dfs
  rename("year_mvp" = "player") %>%
  .[,c(1,2)]
mvp$season[45:65] <- gsub("^([0-9]{4})([^[:alnum:]]{1}[0-9]{2})([0-9]{2})$","\\1-20\\3",mvp$season[45:65])
mvp$year_mvp[44] <- "Karl Malone"
mvp$year_mvp[56] <- "Derrick Rose"

#All-star selections
allstars <- read_html("https://en.wikipedia.org/wiki/List_of_NBA_All-Stars") %>%
  html_nodes(xpath = "//*[@id='mw-content-text']/div[1]/table[2]") %>%
  html_table() %>% data.frame() %>% clean_names() %>%
  #clean up player variable, and prepare allstar selections for future use 
  mutate(player = gsub(" +$","",gsub("[^[:alpha:]]"," ",player)),
         selections = strsplit(selections_c,split = ";"),
         selections_c = NULL) %>%
  .[,c(1,5)]
#manually fix minor issues from webscraping
allstars$player[1] <- "Kareem Abdul Jabbar"
allstars$player[19] <- "Hakeem Olajuwon"
allstars$player[430] <- "Metta World Peace"
```

Here, we have 5 datasets.
- playerstats_df: Years a player was in the NBA, their position, physical characteristics, college, and birth date
- players_df: Mostly identical variables to playerstats_df, just with birth city and state included
- seasonstats_df: Each player's season statistics by year.
- mvp: Webscrapped from wikipedia, this dataframe has a list of mvp's from each season
- allstars: Webscrapped from wikipedia, this dataframe has a list of all players chosen for an allstar game, along with the years they were selected

```{r data wrangling}
#merging the player df's
df_halffull <- left_join(seasonstats_df, players_df,playerstats_df, by = "player")

#create mvp column with TRUE/FALSE values
full_df <- left_join(df_halffull, mvp, by ="season") %>%
  mutate(mvp = (year_mvp == player))

#Create allstar column with TRUE/FALSE values
full_df$allstar <- FALSE
for (i in 1:nrow(allstars)) {
  
  #Pickup selections in their source format from webscrapped table
  selections <- allstars$selections[[i]]
  
  #Turn the format into yyyy:yyyy for further use
  selections_seq <- gsub("[^[:alnum:]]",":",gsub(" ","",selections))
  
  #put years selected into a useable format: a df
  #player= player selected; selections = years selected for allstar game
  years_selected <- data.frame(
    player = allstars$player[i],
    selections = unlist(lapply(selections_seq,function(x) {eval(parse(text = x))}))
  )
  
  #turn values of full_df's allstar column to true based on membership in years_selected
  full_df[(full_df$player == years_selected$player[1] & 
             full_df$year %in% years_selected$selections),"allstar"] <- TRUE
}

#create final df with finalized variables
final_df <- full_df %>%
  #create an age column
  mutate(age = year-born) %>%
  #reorder columns for convenience
  .[,c(2:4,63:64,56:59,60,61,5:54)] %>%
  select(-c(blanl,blank2)) %>%
  mutate(mvp = as_factor(as.numeric(mvp)), allstar = as_factor(as.numeric(allstar)))

#for loop creating dummy variables for different positions
for (i in c("C","PG","PF","SG","SF")) {
  eval(parse(text = paste("final_df$",i,"<- as.numeric(grepl('",i,"',final_df$pos))",sep = "")))
}
```
NOTE: There wasn't an all-star game in the 1998-1999 season, and there were fewer games due to a lockout.


```{r elasticnet}
set.seed(8237)

#split df into training and testing sets by randomly choosing years, each as their own sample
#32 years possible, we will be using 80% (rounded up to 26 total years) for our training and the remainder for testing.
train_years <- sample(unique(final_df$year),26) %>%
  sort()
test_years <- unique(final_df$year)[!unique(final_df$year) %in% train_years] %>%
  sort()

final_train <- final_df[final_df$year %in% train_years,]
final_test <- final_df[final_df$year %in% test_years,]

final_cv <- final_train %>% vfold_cv(v = 3,strata = "year")


#Create recipe and prepare it for use
final_recipe <- final_train %>% recipe(allstar ~ .) %>%
  step_rm(player) %>% step_rm(collage) %>% step_rm(pos) %>%
  step_rm(birth_city) %>% step_rm(birth_state) %>%
  step_normalize(all_predictors() & all_numeric()) %>%
  step_dummy(all_predictors() & all_nominal()) %>%
  step_modeimpute(all_predictors()&all_nominal()) %>%
  step_meanimpute(all_predictors()&all_numeric())
final_clean <- final_recipe %>% prep() %>% juice()

#prepare elasticnet logit model
model_en <- logistic_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet")

#prepare elasticnet logit workflow
workflow_en = workflow() %>%
  add_model(model_en) %>%
  add_recipe(final_recipe)

#calculate the best models
cv_en = workflow_en %>%
  tune_grid(
    final_cv,
    grid = grid_regular(mixture(), penalty(), levels = 3),
    metrics = metric_set(accuracy)
  )

final_en = workflow_en %>%
  finalize_workflow(select_best(cv_en, 'accuracy'))

#Fitting the final model
final_fit_en = final_en %>% fit(data = final_train)

#Predict onto the test data
y_hat = final_fit_en %>% predict(new_data = final_test, type ="class")

cm_logistic = conf_mat(
  data = tibble(
    y_hat = y_hat %>% unlist(),
    y = final_test$mvp
  ),
  truth = y, estimate = y_hat
)

cm_logistic

###NOTE: maybe we use p_hat instead of y_hat? probabilities instead of levels?
```

```{r tree}
#decision tree model
model_tree <- decision_tree(
  mode = "classification",
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = 10
) %>%
  set_engine("rpart")

#setup decision tree workflow
tree_workflow <- workflow() %>%
  add_model(model_tree) %>% 
  add_recipe(final_recipe)

#tune models
tree_cv_fit <- tree_workflow %>% tune_grid(
  final_cv,
  grid = expand_grid(
    cost_complexity = seq(0,.15,by = .05),
    tree_depth = c(1,5,10)
  ),
  metrics = metric_set(accuracy, roc_auc)
)

#select the best model based on accuracy
best_flow_tree <- tree_workflow %>%
  finalize_workflow(select_best(tree_cv_fit, metric = "accuracy")) %>%
  fit(data = final_df)

#pull the best decision tree model
best_tree <- best_flow_tree %>% pull_workflow_fit()

best_tree$fit %>% rpart.plot()
```

```{elasticnet with linear reg}
#set seed
set.seed(101)
#cv 
final_cv = final_train %>%vfold_cv(v=5)

#Elasticnet regression

model_en = linear_reg(penalty = tune(), mixture = tune()) %>% set_engine("glmnet")

#Define the workflow
workflow_en = workflow() %>% 
  add_model(model_en) %>% 
  add_recipe(final_recipe)

cv_en = workflow_en %>%
  tune_grid(
    final_cv,
    grid = grid_regular(mixture(), penalty(), levels = 5:5),
    metrics = metric_set(rmse)
  )

cv_en %>% collect_metrics() %>% arrange(mean)

```





